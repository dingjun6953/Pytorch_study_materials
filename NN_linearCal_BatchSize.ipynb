{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_linearCal_BatchSize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Yow1AhHILrzW",
        "outputId": "d3a61046-8914-46fc-f79e-1f4a91dd9ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.214, 6.335, -1.62]\n",
            "[1.214, 6.335, -1.62]\n"
          ]
        }
      ],
      "source": [
        "#using pure Python to calculate W*x+B\n",
        "#there is no any import library/module\n",
        "#  4 inputs into 3 neuron of outputs\n",
        "inputs=[1,2,3,4]\n",
        "weights=[[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]]\n",
        "biases=[1.02,2,1.5]\n",
        "output_py=[inputs[0]*weights[0][0]+inputs[1]*weights[0][1]+inputs[2]*weights[0][2]+inputs[3]*weights[0][3]+biases[0],\n",
        "           inputs[0]*weights[1][0]+inputs[1]*weights[1][1]+inputs[2]*weights[1][2]+inputs[3]*weights[1][3]+biases[1],\n",
        "           inputs[0]*weights[2][0]+inputs[1]*weights[2][1]+inputs[2]*weights[2][2]+inputs[3]*weights[2][3]+biases[2],\n",
        "]\n",
        "print(output_py)\n",
        "\n",
        "#if there are many inputs and weights, then coding as follow:\n",
        "layer_outputs=[] #output current layer\n",
        "for neuron_weights, neuron_bias in zip(weights,biases):\n",
        "  neuron_output=0 # output of a given neuron\n",
        "  for n_input,weight in zip(inputs,neuron_weights):\n",
        "    neuron_output += n_input * weight\n",
        "  neuron_output +=neuron_bias\n",
        "  layer_outputs.append(neuron_output)\n",
        "print(layer_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WhKCOtA5UTX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using Numpy to calculate W*x+B\n",
        "import numpy as np\n",
        "#tensorflow calculats w*x+b via numpy and then convert the result into the tensor\n",
        "import tensorflow as tf\n",
        "inputs=np.array([1,2,3,4])\n",
        "weights=np.array([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]])\n",
        "biases=np.array([1.02,2,1.5])\n",
        "\n",
        "output_np=np.dot(weights,inputs)+biases\n",
        "output_tf=tf.convert_to_tensor(output_np)\n",
        "print(output_np)\n",
        "print(output_tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wYrEjG74Lsdi",
        "outputId": "3af986e8-70c8-48ed-b3cf-0b6d153807fe"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.214  6.335 -1.62 ]\n",
            "tf.Tensor([ 1.214  6.335 -1.62 ], shape=(3,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Pytorch to calculate W*x+B\n",
        "import torch\n",
        "\n",
        "inputs=torch.tensor([1.,2.,3.,4.],dtype=torch.float32)\n",
        "weights=torch.tensor([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]],dtype=torch.float32)\n",
        "biases=torch.tensor([1.02,2,1.5],dtype=torch.float32)\n",
        "print(weights.shape)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "inputs=inputs.view(inputs.shape[0],1)\n",
        "biases=biases.view(biases.shape[0],1)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "\n",
        "output_torch=torch.matmul(weights,inputs)+biases\n",
        "print(output_torch)\n",
        "print(output_torch.shape)\n",
        "print(\"after reshape of output_torch\")\n",
        "output_torch=output_torch.view(output_torch.shape[0])\n",
        "print(output_torch)\n",
        "print(output_torch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DRg9YkDtLsgR",
        "outputId": "5964c2cf-5eba-4ffe-9baf-7bf77b141d10"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([4])\n",
            "torch.Size([3])\n",
            "torch.Size([4, 1])\n",
            "torch.Size([3, 1])\n",
            "tensor([[ 1.2140],\n",
            "        [ 6.3350],\n",
            "        [-1.6200]])\n",
            "torch.Size([3, 1])\n",
            "after reshape of output_torch\n",
            "tensor([ 1.2140,  6.3350, -1.6200])\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2, 3)\n",
        "b = torch.rand(3, 2)\n",
        "\n",
        "#print(a * b)\n",
        "print(torch.matmul(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zhjiNmq_LsjR",
        "outputId": "75e773d2-9a54-4a06-e40c-c40d9fce7fbf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5915, 0.3654],\n",
            "        [1.2229, 0.6001]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using tensorflow to calculate W*x+B\n",
        "import tensorflow as tf\n",
        "inputs=tf.Variable([[1,2,3,4]],dtype=tf.float32)\n",
        "weights=tf.Variable([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]],dtype=tf.float32)\n",
        "biases=tf.Variable([[1.02,2,1.5]],dtype=tf.float32)\n",
        "print(weights.shape)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "#need to transpose matrix\n",
        "print(tf.transpose(inputs).shape)\n",
        "print(tf.transpose(biases).shape)\n",
        "output_tf=tf.matmul(weights,tf.transpose(inputs))+tf.transpose(biases)\n",
        "print(output_tf)\n",
        "\n",
        "#flatten for vector\n",
        "output_tf_flatten=tf.reshape(output_tf,[-1])\n",
        "print(output_tf_flatten.shape)\n",
        "print(output_tf_flatten)\n",
        "\n",
        "print(\"\\nndim size compared\")\n",
        "print(output_tf.ndim)\n",
        "print(output_tf_flatten.ndim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T1uf6SxhLslx",
        "outputId": "57f63e43-4209-4f41-db80-760895d9d630"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "(1, 4)\n",
            "(1, 3)\n",
            "(4, 1)\n",
            "(3, 1)\n",
            "tf.Tensor(\n",
            "[[ 1.214    ]\n",
            " [ 6.335    ]\n",
            " [-1.6200001]], shape=(3, 1), dtype=float32)\n",
            "(3,)\n",
            "tf.Tensor([ 1.214      6.335     -1.6200001], shape=(3,), dtype=float32)\n",
            "\n",
            "ndim size compared\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
        "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oFPj6EfrLsra",
        "outputId": "c4597050-8c3e-4933-b5a3-c4e80daa6207"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 58  64]\n",
            " [139 154]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable([[1.], [2.]])\n",
        "x = tf.Variable([[3., 4.]])\n",
        "#x = tf.constant([[3., 4.]])\n",
        "print(w.shape)\n",
        "print(x.shape)\n",
        "tf.matmul(w, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LUs2GzpwLsuR",
        "outputId": "f77e4abf-e54c-4696-d723-898140bddff2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1)\n",
            "(1, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[3., 4.],\n",
              "       [6., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g-BTqN1oUVDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "##Hidden Layer Activation Functions\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 5 neurons and the 2nd hidden layer has 2 output neurons\n",
        "#X=[[1.,2.,3.,2.5],\n",
        " #  [2.0,5.0,-1,-0.8],\n",
        "  # [0.7,1.7,2,9,-3.2]]\n",
        "inputs=np.array([1,2,3,4])\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "  \n",
        "class Activation_Relu:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "#you may design the number of hidden layer neurons what you want \n",
        "print(\"\\nThe 1st hidder layer outputs\\n\")\n",
        "layer1=Layer_dense(4,6)\n",
        "#activation1=Activation_Relu()\n",
        "activation1=Activation_Softmax()\n",
        "layer1.forward(inputs)\n",
        "#layer1.forward(X)\n",
        "print(layer1.output)\n",
        "activation1.forward(layer1.output)\n",
        "print(activation1.output)\n",
        "print(np.sum(activation1.output))\n",
        "print(\"\\nThe 2nd hidder layer outputs\\n\")\n",
        "\n",
        "layer2=Layer_dense(6,10)\n",
        "#activation2=Activation_Relu()\n",
        "activation2=Activation_Softmax()\n",
        "layer2.forward(activation1.output)\n",
        "print(layer2.output)\n",
        "activation2.forward(layer2.output)\n",
        "print(activation2.output)\n",
        "print(np.sum(activation2.output))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9x_mD3HUVGv",
        "outputId": "69ec69d6-136e-46ec-e43e-c6ea717e5350"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The 1st hidder layer outputs\n",
            "\n",
            "[[ 0.71996132 -0.29539151 -0.81080693  0.66775876  1.00956271 -0.16528657]]\n",
            "[[0.23384771 0.08471708 0.05059748 0.22195342 0.31239596 0.09648834]]\n",
            "1.0\n",
            "\n",
            "The 2nd hidder layer outputs\n",
            "\n",
            "[[ 0.0563509  -0.04455413 -0.05269866  0.02532824 -0.02103229  0.0248367\n",
            "  -0.01318179  0.00351179 -0.01576078 -0.12748163]]\n",
            "[[0.10742829 0.0971172  0.09632944 0.10414674 0.09942865 0.10409556\n",
            "  0.10021229 0.10189923 0.09995418 0.08938841]]\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CVMVhOR7UVJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Hidden Layer Activation Functions"
      ],
      "metadata": {
        "id": "-A6S-u9wUVM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 5 neurons and the 2nd hidden layer has 2 output neurons\n",
        "X=[[1.,2.,3.,2.5],\n",
        "   [2.0,5.0,-1,-0.8],\n",
        "   [0.7,1.7,2,9,-3.2]]\n",
        "#inputs=np.array([1,2,3,4])\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "#you may design the number of hidden layer neurons what you want \n",
        "layer1=Layer_dense(4,6)\n",
        "layer2=Layer_dense(6,2)\n",
        "\n",
        "#layer1.forward(inputs)\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "n892A-b5UVPj",
        "outputId": "e144601c-2180-4a68-e6f0-251b7b78be29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a91414fa505c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#layer1.forward(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-a91414fa505c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#you may design the number of hidden layer neurons what you want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (4,6) not aligned: 3 (dim 0) != 4 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "05jpJ--mUVSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aOXp3Tl1UVVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6V0TTzGeUVXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}