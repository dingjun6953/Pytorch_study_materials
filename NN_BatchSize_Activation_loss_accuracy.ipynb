{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_BatchSize_Activation_loss_accuracy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yow1AhHILrzW",
        "outputId": "d3a61046-8914-46fc-f79e-1f4a91dd9ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.214, 6.335, -1.62]\n",
            "[1.214, 6.335, -1.62]\n"
          ]
        }
      ],
      "source": [
        "#using pure Python to calculate W*x+B\n",
        "#there is no any import library/module\n",
        "#  4 inputs into 3 neuron of outputs\n",
        "inputs=[1,2,3,4]\n",
        "weights=[[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]]\n",
        "biases=[1.02,2,1.5]\n",
        "output_py=[inputs[0]*weights[0][0]+inputs[1]*weights[0][1]+inputs[2]*weights[0][2]+inputs[3]*weights[0][3]+biases[0],\n",
        "           inputs[0]*weights[1][0]+inputs[1]*weights[1][1]+inputs[2]*weights[1][2]+inputs[3]*weights[1][3]+biases[1],\n",
        "           inputs[0]*weights[2][0]+inputs[1]*weights[2][1]+inputs[2]*weights[2][2]+inputs[3]*weights[2][3]+biases[2],\n",
        "]\n",
        "print(output_py)\n",
        "\n",
        "#if there are many inputs and weights, then coding as follow:\n",
        "layer_outputs=[] #output current layer\n",
        "for neuron_weights, neuron_bias in zip(weights,biases):\n",
        "  neuron_output=0 # output of a given neuron\n",
        "  for n_input,weight in zip(inputs,neuron_weights):\n",
        "    neuron_output += n_input * weight\n",
        "  neuron_output +=neuron_bias\n",
        "  layer_outputs.append(neuron_output)\n",
        "print(layer_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WhKCOtA5UTX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using Numpy to calculate W*x+B\n",
        "import numpy as np\n",
        "#tensorflow calculats w*x+b via numpy and then convert the result into the tensor\n",
        "import tensorflow as tf\n",
        "inputs=np.array([1,2,3,4])\n",
        "weights=np.array([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]])\n",
        "biases=np.array([1.02,2,1.5])\n",
        "\n",
        "output_np=np.dot(weights,inputs)+biases\n",
        "output_tf=tf.convert_to_tensor(output_np)\n",
        "print(output_np)\n",
        "print(output_tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYrEjG74Lsdi",
        "outputId": "3af986e8-70c8-48ed-b3cf-0b6d153807fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.214  6.335 -1.62 ]\n",
            "tf.Tensor([ 1.214  6.335 -1.62 ], shape=(3,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Pytorch to calculate W*x+B\n",
        "import torch\n",
        "\n",
        "inputs=torch.tensor([1.,2.,3.,4.],dtype=torch.float32)\n",
        "weights=torch.tensor([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]],dtype=torch.float32)\n",
        "biases=torch.tensor([1.02,2,1.5],dtype=torch.float32)\n",
        "print(weights.shape)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "inputs=inputs.view(inputs.shape[0],1)\n",
        "biases=biases.view(biases.shape[0],1)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "\n",
        "output_torch=torch.matmul(weights,inputs)+biases\n",
        "print(output_torch)\n",
        "print(output_torch.shape)\n",
        "print(\"after reshape of output_torch\")\n",
        "output_torch=output_torch.view(output_torch.shape[0])\n",
        "print(output_torch)\n",
        "print(output_torch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRg9YkDtLsgR",
        "outputId": "5964c2cf-5eba-4ffe-9baf-7bf77b141d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([4])\n",
            "torch.Size([3])\n",
            "torch.Size([4, 1])\n",
            "torch.Size([3, 1])\n",
            "tensor([[ 1.2140],\n",
            "        [ 6.3350],\n",
            "        [-1.6200]])\n",
            "torch.Size([3, 1])\n",
            "after reshape of output_torch\n",
            "tensor([ 1.2140,  6.3350, -1.6200])\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2, 3)\n",
        "b = torch.rand(3, 2)\n",
        "\n",
        "#print(a * b)\n",
        "print(torch.matmul(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhjiNmq_LsjR",
        "outputId": "75e773d2-9a54-4a06-e40c-c40d9fce7fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5915, 0.3654],\n",
            "        [1.2229, 0.6001]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using tensorflow to calculate W*x+B\n",
        "import tensorflow as tf\n",
        "inputs=tf.Variable([[1,2,3,4]],dtype=tf.float32)\n",
        "weights=tf.Variable([[0.8,-0.76,0.99,-0.514],\n",
        "         [-0.3,-0.45,0.765,0.81],\n",
        "         [0.23,0.38,-0.41,-0.72]],dtype=tf.float32)\n",
        "biases=tf.Variable([[1.02,2,1.5]],dtype=tf.float32)\n",
        "print(weights.shape)\n",
        "print(inputs.shape)\n",
        "print(biases.shape)\n",
        "#need to transpose matrix\n",
        "print(tf.transpose(inputs).shape)\n",
        "print(tf.transpose(biases).shape)\n",
        "output_tf=tf.matmul(weights,tf.transpose(inputs))+tf.transpose(biases)\n",
        "print(output_tf)\n",
        "\n",
        "#flatten for vector\n",
        "output_tf_flatten=tf.reshape(output_tf,[-1])\n",
        "print(output_tf_flatten.shape)\n",
        "print(output_tf_flatten)\n",
        "\n",
        "print(\"\\nndim size compared\")\n",
        "print(output_tf.ndim)\n",
        "print(output_tf_flatten.ndim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1uf6SxhLslx",
        "outputId": "57f63e43-4209-4f41-db80-760895d9d630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "(1, 4)\n",
            "(1, 3)\n",
            "(4, 1)\n",
            "(3, 1)\n",
            "tf.Tensor(\n",
            "[[ 1.214    ]\n",
            " [ 6.335    ]\n",
            " [-1.6200001]], shape=(3, 1), dtype=float32)\n",
            "(3,)\n",
            "tf.Tensor([ 1.214      6.335     -1.6200001], shape=(3,), dtype=float32)\n",
            "\n",
            "ndim size compared\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
        "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
        "c = tf.matmul(a, b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFPj6EfrLsra",
        "outputId": "c4597050-8c3e-4933-b5a3-c4e80daa6207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 58  64]\n",
            " [139 154]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable([[1.], [2.]])\n",
        "x = tf.Variable([[3., 4.]])\n",
        "#x = tf.constant([[3., 4.]])\n",
        "print(w.shape)\n",
        "print(x.shape)\n",
        "tf.matmul(w, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUs2GzpwLsuR",
        "outputId": "f77e4abf-e54c-4696-d723-898140bddff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1)\n",
            "(1, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[3., 4.],\n",
              "       [6., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g-BTqN1oUVDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "#Hidden Layer Activation Functions\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 5 neurons and the 2nd hidden layer has 2 output neurons\n",
        "X=[[1.,2.,3.,2.5],\n",
        "   [2.0,5.0,-1,-0.8],\n",
        "   [0.7,1.7,2.9,-3.2]]\n",
        "\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "  \n",
        "class Activation_Relu:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "#you may design the number of hidden layer neurons what you want \n",
        "print(\"\\nThe 1st hidder layer outputs\\n\")\n",
        "layer1=Layer_dense(4,6)\n",
        "#activation1=Activation_Relu()\n",
        "activation1=Activation_Softmax()\n",
        "#layer1.forward(inputs)\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "activation1.forward(layer1.output)\n",
        "print(activation1.output)\n",
        "#print(np.sum(activation1.output))\n",
        "print(\"\\nThe 2nd hidder layer outputs\\n\")\n",
        "\n",
        "layer2=Layer_dense(6,3)\n",
        "#activation2=Activation_Relu()\n",
        "activation2=Activation_Softmax()\n",
        "layer2.forward(activation1.output)\n",
        "print(layer2.output)\n",
        "activation2.forward(layer2.output)\n",
        "print(activation2.output)\n",
        "#print(np.sum(activation2.output))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9x_mD3HUVGv",
        "outputId": "a3796220-d994-4807-d663-2d3e9956e909"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The 1st hidder layer outputs\n",
            "\n",
            "[[ 0.67300116 -0.16727715 -0.42785846  0.56971597  0.87989728 -0.05396182]\n",
            " [ 0.72670549  0.060513    0.30399103  0.56782097  0.22697058  0.61157021]\n",
            " [ 0.40551797  0.31087667  0.99664153  0.11427187  0.31187981  0.35681395]]\n",
            "[[0.22834831 0.09855294 0.07594524 0.20594043 0.28083527 0.11037781]\n",
            " [0.22124098 0.11364278 0.14497163 0.18873955 0.13422502 0.19718004]\n",
            " [0.15807471 0.14380044 0.28548553 0.11813425 0.14394476 0.15056031]]\n",
            "\n",
            "The 2nd hidder layer outputs\n",
            "\n",
            "[[ 0.04158301 -0.00020379 -0.01454795]\n",
            " [ 0.02350267 -0.02037822 -0.0254077 ]\n",
            " [ 0.02736881  0.00725813 -0.02860041]]\n",
            "[[0.34429458 0.33020406 0.32550137]\n",
            " [0.34372123 0.32896457 0.3273142 ]\n",
            " [0.3418034  0.33499816 0.32319844]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CVMVhOR7UVJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 6 neurons and the 2nd hidden layer has 2 output neurons\n",
        "X=[[1.,2.,3.,2.5],\n",
        "   [2.0,5.0,-1,-0.8],\n",
        "   [0.7,1.7,2.9,-3.2]]\n",
        "\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "#you may design the number of hidden layer neurons what you want \n",
        "layer1=Layer_dense(4,6)\n",
        "layer2=Layer_dense(6,2)\n",
        "\n",
        "#layer1.forward(inputs)\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n892A-b5UVPj",
        "outputId": "86561761-382c-4b72-8c79-9062521d3f7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.67300116 -0.16727715 -0.42785846  0.56971597  0.87989728 -0.05396182]\n",
            " [ 0.72670549  0.060513    0.30399103  0.56782097  0.22697058  0.61157021]\n",
            " [ 0.40551797  0.31087667  0.99664153  0.11427187  0.31187981  0.35681395]]\n",
            "[[ 0.01899703 -0.31120449]\n",
            " [ 0.17918717 -0.07607862]\n",
            " [ 0.20789667  0.02976919]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "05jpJ--mUVSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias=np.zeros((1,6))\n",
        "bias_tf=tf.convert_to_tensor(bias)\n",
        "bias_tf=tf.transpose(bias_tf)\n",
        "print(bias_tf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOXp3Tl1UVVA",
        "outputId": "63acfcf5-82cc-49b4-9087-687f2799ea4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 6 neurons and the 2nd hidden layer has 2 output neurons\n",
        "X=[[1.,2.,3.,2.5],\n",
        "   [2.0,5.0,-1,-0.8],\n",
        "   [0.7,1.7,2.9,3.2]]\n",
        "\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "    \n",
        "#you may design the number of hidden layer neurons what you want \n",
        "layer1=Layer_dense(4,6)\n",
        "layer2=Layer_dense(6,2)\n",
        "outputs=[[]]\n",
        "for inputs in X: \n",
        "  layer1.forward(inputs)\n",
        "  print(layer1.output)\n",
        "  layer2.forward(layer1.output)\n",
        "  print(layer2.output)\n",
        "  outputs.append(layer2.output)\n",
        "print(outputs)\n",
        "print(outputs[0])\n",
        "print(outputs[1])\n",
        "print(outputs[2])\n",
        "print(outputs[3])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V0TTzGeUVXv",
        "outputId": "27b95b03-3468-4b3e-c36f-64ca85865889"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.67300116 -0.16727715 -0.42785846  0.56971597  0.87989728 -0.05396182]]\n",
            "[[ 0.01899703 -0.31120449]]\n",
            "[[0.72670549 0.060513   0.30399103 0.56782097 0.22697058 0.61157021]]\n",
            "[[ 0.17918717 -0.07607862]]\n",
            "[[ 0.6058813  -0.2357446  -0.63727195  0.53258777  0.86511898 -0.11817166]]\n",
            "[[-0.02567865 -0.33041224]]\n",
            "[[], array([[ 0.01899703, -0.31120449]]), array([[ 0.17918717, -0.07607862]]), array([[-0.02567865, -0.33041224]])]\n",
            "[]\n",
            "[[ 0.01899703 -0.31120449]]\n",
            "[[ 0.17918717 -0.07607862]]\n",
            "[[-0.02567865 -0.33041224]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cCCqYnno1Ue9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E-W_zLFI1Um0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batches, Layers, and Objects\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "#inputs [3,4], batchsize=3\n",
        "#define hidden layer as an object\n",
        "# The object has 3 features of weights, biases and output\n",
        "# hidden layer size: the 1st hidden layer has 6 neurons and the 2nd hidden layer has 2 output neurons\n",
        "X=[[1.,2.,3.,2.5],\n",
        "   [2.0,5.0,-1,-0.8],\n",
        "   [0.7,1.7,2.9,3.2]]\n",
        "\n",
        "class Layer_dense:\n",
        "  def __init__(self, n_inputs,n_neurons):\n",
        "    self.weights=0.10*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "    \n",
        "#you may design the number of hidden layer neurons what you want \n",
        "layer1=Layer_dense(4,6)\n",
        "layer2=Layer_dense(6,2)\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)\n",
        "outputs.append(layer2.output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnMkktHOAwNM",
        "outputId": "ae8e80fb-5c34-43e1-a344-cb78cdf3361c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.67300116 -0.16727715 -0.42785846  0.56971597  0.87989728 -0.05396182]\n",
            " [ 0.72670549  0.060513    0.30399103  0.56782097  0.22697058  0.61157021]\n",
            " [ 0.6058813  -0.2357446  -0.63727195  0.53258777  0.86511898 -0.11817166]]\n",
            "[[ 0.01899703 -0.31120449]\n",
            " [ 0.17918717 -0.07607862]\n",
            " [-0.02567865 -0.33041224]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "APVK5aqH1Vtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Loss with Categorical Cross-Entropy\n",
        "import math\n",
        "softmax_output=[0.7,0.2,0.1]\n",
        "target_label=[1,0,0] # one hot coding, class=3, label=0\n",
        "loss=-(math.log(softmax_output[0])*target_label[0]+\n",
        "       math.log(softmax_output[1])*target_label[1]+\n",
        "       math.log(softmax_output[2])*target_label[2])\n",
        "print(loss)\n",
        "\n",
        "loss=-math.log(0.7)\n",
        "print(loss)\n",
        "#when high probability, corresponding to low loss. \n",
        "loss=-math.log(0.9)\n",
        "print(loss)\n",
        "      \n",
        "loss=-math.log(0.1)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwsZNK-Z1V1b",
        "outputId": "9d86fb89-5f87-491b-e28b-4f68549dcde5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n",
            "0.35667494393873245\n",
            "0.10536051565782628\n",
            "2.3025850929940455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Loss with Categorical Cross-Entropy\n",
        "\n",
        "#Implementing Loss\n",
        "\n"
      ],
      "metadata": {
        "id": "ybGwMKLM1V79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a=[[0.7,0.2,0.1],[0.1,0.5,0.4],[0.02,0.9,0.08]]\n",
        "b=[[1.,0.,0.],[0.,1.,0.],[0.,1.,0.]]\n",
        "c=np.matmul(a,b)\n",
        "d=np.dot(a,b)\n",
        "print(c)\n",
        "print(d)\n",
        "\n",
        "sum=np.sum(c,axis=1)\n",
        "print(sum)\n",
        "\n",
        "sum=np.sum(c,axis=0)\n",
        "print(sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtSjYZJt1V_-",
        "outputId": "302283c0-b6fa-46dc-e1d9-fa6ff2f99635"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7  0.3  0.  ]\n",
            " [0.1  0.9  0.  ]\n",
            " [0.02 0.98 0.  ]]\n",
            "[[0.7  0.3  0.  ]\n",
            " [0.1  0.9  0.  ]\n",
            " [0.02 0.98 0.  ]]\n",
            "[1. 1. 1.]\n",
            "[0.82 2.18 0.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JErLg7QSCSmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metric for accuracy calculation\n",
        "import numpy as np\n",
        "softmax_output=np.array([[0.7,0.2,0.1],\n",
        "                         [0.1,0.5,0.4],\n",
        "                         [0.02,0.9,0.08]])\n",
        "class_targets=[0,1,1]\n",
        "prediction=np.argmax(softmax_output,axis=1)\n",
        "print(prediction)\n",
        "accuracy=np.mean(prediction==class_targets)\n",
        "print(\"acc:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2wNgCUYCTRB",
        "outputId": "248ff0a7-2b08-4b8c-d3a3-bae08fa95332"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1]\n",
            "acc: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "softmax_output=np.array([[0.7,0.2,0.1],\n",
        "                         [0.5,0.1,0.4],\n",
        "                         [0.02,0.9,0.08]])\n",
        "class_targets=[0,1,1]\n",
        "prediction=np.argmax(softmax_output,axis=1)\n",
        "print(prediction)\n",
        "accuracy=np.mean(prediction==class_targets)\n",
        "print(\"acc:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR6vo2AaD8K7",
        "outputId": "5ec57642-3854-4cc9-fd96-9187e8f45c89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n",
            "acc: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}